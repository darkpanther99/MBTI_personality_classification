{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MBTI Classification","metadata":{}},{"cell_type":"markdown","source":"## 0. Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom torchtext.vocab import GloVe,vocab\n# from transformers import (set_seed,\n#                           TrainingArguments,\n#                           Trainer,\n#                           GPT2Config,\n#                           GPT2Tokenizer,\n#                           AdamW,\n#                           get_linear_schedule_with_warmup,\n#                           GPT2ForSequenceClassification,\n#                           GPT2PreTrainedModel,\n#                           GPT2Model)\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\nfrom typing import Optional, Tuple, Union\n# from transformers.modeling_outputs import SequenceClassifierOutputWithPast","metadata":{"execution":{"iopub.status.busy":"2022-11-14T23:07:32.712161Z","iopub.execute_input":"2022-11-14T23:07:32.712936Z","iopub.status.idle":"2022-11-14T23:07:35.412401Z","shell.execute_reply.started":"2022-11-14T23:07:32.712900Z","shell.execute_reply":"2022-11-14T23:07:35.411421Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## Playground\n\nprint(torch.cuda.is_available())\n\n!ls \"../input/mbti-personality-types-500-dataset/MBTI 500.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-11-14T23:07:35.414264Z","iopub.execute_input":"2022-11-14T23:07:35.414840Z","iopub.status.idle":"2022-11-14T23:07:36.532741Z","shell.execute_reply.started":"2022-11-14T23:07:35.414803Z","shell.execute_reply":"2022-11-14T23:07:36.531580Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"True\n'../input/mbti-personality-types-500-dataset/MBTI 500.csv'\n","output_type":"stream"}]},{"cell_type":"code","source":"# CONSTANTS\n\nMAX_SEQ_LEN = 500\nN_LABELS = 16\nN_LABELS_BINARY = 1","metadata":{"execution":{"iopub.status.busy":"2022-11-14T23:17:18.669994Z","iopub.execute_input":"2022-11-14T23:17:18.670384Z","iopub.status.idle":"2022-11-14T23:17:18.675676Z","shell.execute_reply.started":"2022-11-14T23:17:18.670349Z","shell.execute_reply":"2022-11-14T23:17:18.674515Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 1. Helper methods","metadata":{}},{"cell_type":"code","source":"def convert_personality_type_to_binary(mbti_type):\n    mapper = {\n        'I':0,\n        'E':1,\n        'N':0,\n        'S':1,\n        'T':0,\n        'F':1,\n        'J':0,\n        'P':1,\n    }\n\n    return [mapper[i] for i in mbti_type]\n\ndef convert_personality_type_to_int(mbti_type):\n    types = [\n                'INTJ', 'INTP', 'ISFJ', 'ISFP',\n                'ISTJ', 'ISTP', 'ENFJ', 'ENFP',\n                'ENTJ', 'ENTP','ESFJ', 'ESFP',\n                'ESTJ', 'ESTP', 'INFJ', 'INFP'\n            ]\n    ints = [i for i in range(len(types))]\n    mapper = dict(zip(types, ints))\n\n    return mapper[mbti_type]\n\ndef convert_binary_to_personality_type(binary_mbti_type):\n    mbti_arrays = [['I', 'E'], ['N', 'S'], ['T', 'F'], ['J', 'P']]\n    mbti_string = ''\n    for idx, mbti_type in enumerate(binary_mbti_type):\n        mbti_string += mbti_arrays[idx][int(mbti_type)]\n    return mbti_string","metadata":{"execution":{"iopub.status.busy":"2022-11-14T23:07:42.135274Z","iopub.execute_input":"2022-11-14T23:07:42.136211Z","iopub.status.idle":"2022-11-14T23:07:42.145229Z","shell.execute_reply.started":"2022-11-14T23:07:42.136172Z","shell.execute_reply":"2022-11-14T23:07:42.144198Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MBTIDataset(Dataset):\n    def __init__(self, data_path, vectorizing_method = None, binary_outputs = False, max_seq_len=500):\n        \"\"\"\n        Vectorizing methods:\n        None - returns raw text\n        basic - basic builtin pytorch vectorizer\n        TfIdf - tf-idf vectorizer\n        GloVe - Global Vectors pretrained embedding\n        \"\"\"\n        self.df = pd.read_csv(data_path)\n        self.vectorizing_method = vectorizing_method\n        self.max_seq_len = max_seq_len\n        self.split_dataframe(self.max_seq_len)\n\n        if vectorizing_method:\n            if vectorizing_method.lower == 'basic':\n                self.tokenizer = get_tokenizer('basic_english')\n                self.vocab = build_vocab_from_iterator(self.yield_tokens_from_dataframe(), specials=['<unk>'])\n                self.vocab.set_default_index(self.vocab[\"<unk>\"])\n\n            if vectorizing_method.lower == 'tfidf' or vectorizing_method.lower == 'tf-idf':\n                self.tokenizer = TfidfVectorizer(stop_words= 'english')\n                self.vocab = self.tokenizer.fit_transform(self.df['posts']) # Sparse matrix representation - could use different field names\n            if vectorizing_method.lower == 'glove':\n                unk_index = 0\n                self.global_vectors = GloVe(name='6B', dim=50)\n                self.vocab=vocab(self.global_vectors.stoi)\n                self.vocab.insert_token(\"<unk>\",unk_index)\n                self.vocab.set_default_index(unk_index)\n\n                self.pretrained_embeddings = self.global_vectors.vectors\n                self.pretrained_embeddings = torch.cat((torch.zeros(1,self.pretrained_embeddings.shape[1]),self.pretrained_embeddings))\n\n\n        self.binary_outputs = binary_outputs\n        if binary_outputs:\n            self.df['type'] = self.df['type'].apply(convert_personality_type_to_binary)\n        else:\n            self.df['type'] = self.df['type'].apply(convert_personality_type_to_int)\n\n    def yield_tokens_from_dataframe(self):\n        for post in self.df['posts']:\n            yield self.tokenizer(post)\n\n    def split_dataframe(self, new_seq_len):\n        new_df = pd.DataFrame(columns=self.df.columns)\n        new_posts = []\n        new_types = []\n        for idx, row in self.df.iterrows():\n            split_posts = row['posts'].split(' ')\n            i = 0\n            while i < len(split_posts):\n                new_posts.append((' ').join(split_posts[i:i+new_seq_len]))\n                new_types.append(row['type'])\n                i += new_seq_len\n\n        new_df['posts'] = new_posts\n        new_df['type'] = new_types\n        self.df = new_df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if idx >= len(self): raise IndexError\n\n        if not self.vectorizing_method :\n            return self.df['posts'][idx], self.df['type'][idx]  #Return raw text\n\n        input_text = self.vocab(self.df['posts'][idx].split(' '))\n        if len(input_text) < self.max_seq_len:\n            input_text.extend([0] * (self.max_seq_len-len(input_text)))\n        label = self.df['type'][idx]\n\n        return input_text[0:self.max_seq_len], label","metadata":{"execution":{"iopub.status.busy":"2022-11-14T23:07:44.695275Z","iopub.execute_input":"2022-11-14T23:07:44.695698Z","iopub.status.idle":"2022-11-14T23:07:44.713012Z","shell.execute_reply.started":"2022-11-14T23:07:44.695664Z","shell.execute_reply":"2022-11-14T23:07:44.711898Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Gpt2ClassificationCollator(object):\n\n    def __init__(self, use_tokenizer, max_sequence_len=None):\n\n        # Tokenizer to be used inside the class.\n        self.use_tokenizer = use_tokenizer\n        # Check max sequence length.\n        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n\n        return\n\n    def __call__(self, sequences):\n\n        # Get all texts from sequences list.\n        texts = [sequence[0] for sequence in sequences]\n        # Get all labels from sequences list.\n        labels = [sequence[1] for sequence in sequences]\n        # Call tokenizer on all texts to convert into tensors of numbers with\n        # appropriate padding.\n        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n        # Update the inputs with the associated encoded labels as tensor.\n        inputs.update({'labels':torch.tensor(labels)})\n\n        return inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"code","source":"# Load data\nmax_seq_len = 500\n\n# Need tokenizer\n# tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='gpt2')\n# tokenizer.padding_side = \"left\"\n# tokenizer.pad_token = tokenizer.eos_token\n# gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, max_sequence_len=max_seq_len)\n\n\nds = MBTIDataset('../input/mbti-personality-types-500-dataset/MBTI 500.csv', vectorizing_method = \"tfidf\", binary_outputs=True, max_seq_len = max_seq_len)\ntrain_set_size = int(len(ds)*0.7)\nval_set_size = int(len(ds)*0.2)\ntest_set_size = len(ds) - train_set_size - val_set_size\ntrain_ds, val_ds, test_ds = random_split(ds, [train_set_size, val_set_size, test_set_size])\n\ntrain_dataloader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=gpt2_classificaiton_collator)\nval_dataloader = DataLoader(val_ds, batch_size=8, shuffle=True, collate_fn=gpt2_classificaiton_collator)\ntest_dataloader = DataLoader(test_ds, batch_size=8, shuffle=True, collate_fn=gpt2_classificaiton_collator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM_model(nn.Module):\n    \n    def __init__(self, hidden_dimension=128):\n        super(LSTM_model, self).__init__()\n        \n        self.dimension = hidden_dimension\n        \n        self.embedding = nn.Embedding(len(text_field.vocab), 300) # Embedding dim 300\n        self.lstm_1 = nn.LSTM(input_size=MAX_SEQ_LEN,\n                            hidden_size=hidden_dimension,\n                            num_layers=1,\n                            batch_first=True,\n                            bidirectional=True)\n        self.lstm_2 = nn.LSTM(input_size=MAX_SEQ_LEN,\n                            hidden_size=hidden_dimension,\n                            num_layers=1,\n                            batch_first=True,\n                            bidirectional=True)\n        \n#         self.max_pooling = nn.MaxPool1d()\n        self.dense1 = nn.Linear(hidden_dimension * 2, out_features=hidden_dimension/2) # total worlds / 2\n        self.dropout = nn.Dropout(0.5)\n        self.dense2 = nn.Linear(hidden_dimension/2, N_LABELS)  # total worlds\n        \n    def forward(self, x):\n        \n        out = self.embedding(x)\n        \n        # Packing output\n        # packed_input = pack_padded_sequence(out, text_len, batch_first=True, enforce_sorted=False)\n        # packed_output, _ = self.lstm(packed_input)\n        # out, _ = pad_packed_sequence(packed_output, batch_first=True)\n        \n#         out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n#         out_reverse = output[:, 0, self.dimension:]\n#         out_reduced = torch.cat((out_forward, out_reverse), 1)\n#         text_fea = self.drop(out_reduced)\n\n#         text_fea = self.fc(text_fea)\n#         text_fea = torch.squeeze(text_fea, 1)\n\n\n#         h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n#         c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n\n#         torch.nn.init.xavier_normal_(h)\n#         torch.nn.init.xavier_normal_(c)\n#         out, (_, _) = self.lstm(out, (h,c))\n        \n        out, (_, _) = self.lstm(out)\n        \n        out = self.drouput(out)\n        out = self.dense1(out)\n        out = torch.tanh(out[:,-1,:])\n        out = self.dropout(out)\n        out = self.dense2(out)\n        out = torch.softmax(out)\n    \n        return out \n\n\n# model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='gpt2', num_labels=n_labels)\n\n# tokenizer.padding_side = \"left\"\n# tokenizer.pad_token = tokenizer.eos_token\n\n# model = GPT2ForBinaryMBTIClassification.from_pretrained(pretrained_model_name_or_path='gpt2', config=model_config)\n# model.resize_token_embeddings(len(tokenizer))\n# model.config.pad_token_id = model.config.eos_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LSTM_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use later\n# class GlobalMaxPooling1D(nn.Module):\n\n# def __init__(self, data_format='channels_last'):\n#     super(GlobalMaxPooling1D, self).__init__()\n#     self.data_format = data_format\n#     self.step_axis = 1 if self.data_format == 'channels_last' else 2\n\n# def forward(self, input):\n#     return torch.max(input, axis=self.step_axis).values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tranining","metadata":{}},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cuda')\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(),\n                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # default is 1e-8.\n                  )\n\n# Total number of training steps is number of batches * number of epochs.\n# `train_dataloader` contains batched data so `len(train_dataloader)` gives\n# us the number of batches.\nepochs = 2\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    \n    predictions_labels = []\n    true_labels = []\n    # Total loss for this epoch.\n    total_loss = 0\n    # For each batch of training data...\n    i = 0\n    i_to_zero_count = 0\n    for batch in tqdm(train_dataloader, total=len(train_dataloader), position=0, leave=True):\n\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n        model.zero_grad()\n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        logits = logits.detach().cpu().numpy()\n        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n        \n        if i % 500 == 0:\n            i=0\n            i_to_zero_count += 1\n            print(total_loss / (i_to_zero_count * 500 * 8))\n                  \n        i += 1\n\n    avg_epoch_loss = total_loss / len(train_dataloader)\n    print(avg_epoch_loss)\n    \n    predictions_labels = []\n    true_labels = []\n    total_loss = 0\n\n    model.eval()\n\n    # Evaluate data for one epoch\n    for batch in tqdm(val_dataloader, total=len(val_dataloader), position=0, leave=True):\n\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n        with torch.no_grad():        \n            outputs = model(**batch)\n            loss, logits = outputs[:2]\n            logits = logits.detach().cpu().numpy()\n            total_loss += loss.item()\n            predict_content = logits.argmax(axis=-1).flatten().tolist()\n            predictions_labels += predict_content\n\n    avg_epoch_loss = total_loss / len(val_dataloader)\n    print(avg_epoch_loss)\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), './manual_model_epoch1_binary.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_labels = []\ntrue_labels = []\ntotal_loss = 0\n\nmodel.eval()\n\n# Evaluate data for one epoch\nfor batch in tqdm(test_dataloader, total=len(test_dataloader)):\n\n    true_labels += batch['labels'].numpy().flatten().tolist()\n    batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n    with torch.no_grad():        \n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        logits = logits.detach().cpu().numpy()\n        total_loss += loss.item()\n        predict_content = logits.argmax(axis=-1).flatten().tolist()\n        predictions_labels += predict_content\n\navg_epoch_loss = total_loss / len(test_dataloader)\nprint(avg_epoch_loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mbti_types = [\n                'INTJ', 'INTP', 'ISFJ', 'ISFP',\n                'ISTJ', 'ISTP', 'ENFJ', 'ENFP',\n                'ENTJ', 'ENTP','ESFJ', 'ESFP',\n                'ESTJ', 'ESTP', 'INFJ', 'INFP'\n            ]\n\nfig, ax = plt.subplots(figsize=(20, 20))\ncm = confusion_matrix(true_labels, predictions_labels, labels=[i for i in range(16)], normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mbti_types)\ndisp.plot(ax=ax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy:', accuracy_score(true_labels, predictions_labels))\nprint('F1 score:', f1_score(true_labels, predictions_labels, average='weighted'))\nprint('Precision:', precision_score(true_labels, predictions_labels, average='weighted'))\nprint('Recall:', recall_score(true_labels, predictions_labels, average='weighted'))","metadata":{},"execution_count":null,"outputs":[]}]}