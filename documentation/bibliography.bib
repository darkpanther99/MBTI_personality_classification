@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{MBTI_class,
	doi = {10.3897/jucs.70941},
  
	url = {https://doi.org/10.3897\%2Fjucs.70941},
  
	year = 2022,
	month = {apr},
  
	publisher = {Pensoft Publishers},
  
	volume = {28},
  
	number = {4},
  
	pages = {378--395},
  
	author = {Vitor dos Santos and Ivandre Paraboni},
  
	title = {Myers-Briggs personality classification from social media text using pre-trained language models},
  
	journal = {{JUCS} - Journal of Universal Computer Science}
}

@misc{DL_text_class,
  doi = {10.48550/ARXIV.2004.03705},
  
  url = {https://arxiv.org/abs/2004.03705},
  
  author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Learning Based Text Classification: A Comprehensive Review},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{gpt-3,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{gpt-neox,
  doi = {10.48550/ARXIV.2204.06745},
  
  url = {https://arxiv.org/abs/2204.06745},
  
  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{BERT,
  doi = {},
  
  url = {},
  
  author = {Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova},
  
  keywords = {},
  
  title = {Google AI Language: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{BertGCN,
  doi = {10.48550/arXiv.2105.05727},
  
  url = {https://arxiv.org/abs/2105.05727},
  
  author = {Yuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han, Kun Kuang, Jiwei Li and Fei Wu},
  
  keywords = {Computation and Language (cs.CL)},
  
  title = {BertGCN: Transductive Text Classification by Combining GCN and BERT},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Fine-tuning,
  doi = {10.48550/arXiv.1801.06146},
  
  url = {https://arxiv.org/abs/1801.06146},
  
  author = {Jeremy Howard, Sebastian Ruder},
  
  keywords = {Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)},
  
  title = {Universal Language Model Fine-tuning for Text Classification},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{roBERTa,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Albert,
  doi = {10.48550/ARXIV.1909.11942},
  
  url = {https://arxiv.org/abs/1909.11942},
  
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
